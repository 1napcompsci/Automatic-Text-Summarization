{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料\n",
    "- 讀取爬文和下載好的100篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news100 = pd.read_excel('appledaily_politinews100.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理文集並創建語料庫\n",
    "- 清理爬下來文集（刪除標點符號，數字，和停止詞）\n",
    "- 設定/創造詞庫\n",
    "- 創造語料庫，然後把它轉換成電腦能分析的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "Building prefix dict from C:\\Users\\YB0002777\\Desktop\\Learning Python\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\YB0002~1\\AppData\\Local\\Temp\\jieba.u018f3a7b0c6b32cba5278025feb505ed.cache\n",
      "Loading model cost 1.301 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "text = news100['content']\n",
    "\n",
    "# 設定適合的標點符號和停止詞\n",
    "punctuation = [' ','；','，','。','！','：','「','」','｣','…','、','？','【','】','.',':','?',';','!','~','`','+','-','<','>','/','[',']','{','}',\"'\",'\"',     '\\xa0','%','）','（','\\u3000','\\u200b','\\t','／']\n",
    "#stopwords = {}.fromkeys([ line.rstrip() for line in open('ting_zhi_words.txt', encoding='utf8') ])\n",
    "stop = requests.get('https://raw.githubusercontent.com/tomlinNTUB/Machine-Learning/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt', verify = False)\n",
    "stopwords = stop.text.split('\\r\\n')\n",
    "addedStops = ['httpswwwfacebookcomappledailytw','nannan','好','已','最']\n",
    "\n",
    "# 刪除標點符號和數字\n",
    "cleanText = []\n",
    "for doc in text:\n",
    "    onedoc=''\n",
    "    for char in str(doc).replace('\\n',''):\n",
    "        if char not in punctuation and not char.isdigit() :#and char not in stopwords\n",
    "            onedoc += char\n",
    "    cleanText.append(onedoc)\n",
    "\n",
    "# 設定詞庫和加詞\n",
    "jieba.set_dictionary('dict.txt.big.txt') # 應用繁體詞庫\n",
    "jieba.add_word('蔡英文') # 加新詞進入詞庫\n",
    "jieba.add_word('韓國瑜')\n",
    "jieba.add_word('柯P')\n",
    "jieba.add_word('柯文哲')\n",
    "jieba.add_word('邱豐光')\n",
    "jieba.add_word('民進黨')\n",
    "jieba.add_word('國民黨')\n",
    "\n",
    "# 創造語料庫，刪除停止詞\n",
    "corpus = [] # 語料庫\n",
    "#cleanList = list(set(noBrakesList))\n",
    "for cleanDoc in cleanText:\n",
    "    textList = jieba.lcut(cleanDoc, cut_all=False)\n",
    "    noBrakesList = []\n",
    "    for phrase in textList:\n",
    "        if phrase not in stopwords and phrase not in addedStops:\n",
    "            noBrakesList.append(phrase)\n",
    "    corpus.append(' '.join(noBrakesList))\n",
    "\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把語料庫和文章轉換成vector\n",
    "vect = TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b', max_features = 6500)\n",
    "corpus_tfidf = vect.fit_transform(corpus)\n",
    "#doc_tfidf = vect.transform([testDoc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Shape:\t (100, 6500)\n"
     ]
    }
   ],
   "source": [
    "# 語料庫vector shape\n",
    "print('Corpus Shape:\\t', corpus_tfidf.shape)\n",
    "#print('Document Shape:\\t', doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['邱豐光', '邱', '開口', '侯', '副', '市長', '目前', '柯文哲', '多以', '邱任', '邱力',\n",
       "       '人脈廣', '必達', '明升暗降', '搶人', '空穴來風', '商界', '看待', '冀望', '看上', '弟',\n",
       "       '探詢', '使命', '將於月', '強邱', '調往', '警政', '正式', '王嘉慶', '警', '去年', '警察局',\n",
       "       '不約而同', '大學', '大戰', '柯侯', '宜', '周刊', '警政署', '人侯友', '副手', '喜歡',\n",
       "       '各縣市', '私下', '回覆', '來當', '早', '署長', '發現', '一場', '就職', '幕僚', '王',\n",
       "       '侯友宜', '做好', '柯p', '局長', '綜合', '意願', '議會', '外界', '交換', '前度', '片',\n",
       "       '邁選', '報導', '效果', '曝光', '挺', '展開', '首長', '協議', '密約', '重要', '找',\n",
       "       '新北', '君子', '結束', '溝通', '當時', '落幕', '對此', '工作', '選戰', '討論', '決定',\n",
       "       '台北市', '原本', '參選', '前', '日', '時', '韓國瑜', '月', '當選', '陳', '選舉',\n",
       "       '知道', '想', '表示'], dtype='<U33')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 語料庫最高TF-IDF分數的100個單詞/最重要的單詞\n",
    "feature_array = np.array(vect.get_feature_names())\n",
    "tfidf_sorting = np.argsort(corpus_tfidf.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 100\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize文章並找出最適合做摘要的句子\n",
    "- 把文章分成句子\n",
    "- 把句子轉換成vector，然後計算每個句子的TF-IDF分數\n",
    "- 找出TF-IDF分數最高的句子用來做長文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今年九合一大選投票狀況多，據了解，中選會官方網站在投票日當天曾遭到來自境外大規模分散式阻斷服務（DD0oS）攻擊，所幸在負責網路維護中華電信應變下，未進一步造成影響。中選會副主委陳朝建今證實曾遭駭客攻擊，不過計票網路是封閉系統，並非公開網路，無法由外界接取，因此沒有任何影響，境外網路干擾也並未影響投票進程。行政院發言人Kolas Yotaka（谷辣斯・尤達卡）表示，中選會在選前即做好縝密防護，因此成功防堵攻擊，接下來將針對有形的選務如投票排隊問題，以及無形的選務如資安防護進行檢討。不過，投票日當天開票進程緩慢，台北市選情更創下史上第一次開票長達9小時的紀錄，陳朝建說明，開票時間長是因為投票時間延後，計票不受駭客影響，開票也自然不受此影響。據內部消息指出，投票日當天曾出現從境外進行分散式阻斷服務（DDoS）攻擊，企圖癱瘓中選會網站，不過在中華電信、中選會緊急處理後，阻斷來自國外異常流量，在短時間獲得解決，一般民眾並沒有發現特殊異樣。NCC及中華電信暫未對此做進一步說明。陳朝建表示，受攻擊的是中選會官方網站，當日開票的計票網路是獨立的封閉網路，並非公開網路，無法由外界接取，完全未有任何影響；中華電信團隊也是24小時監控與防護作業，也沒有受到任何影響。陳朝建說，受到攻擊的官網，當日皆有隨時防衛，中華電信團隊24小時監控與防護作業；故當日遭到來自境外大規模分散式阻斷服務（DD0oS）攻擊時，在負責網路維護中華電信的緊急應變下，並沒有受到影響。另外，Kolas表示，中選會的官網在選舉當天上午6時56分至7時01分期間，遭受大規模境外DDoS攻擊，而中選會的系統是封閉系統，防護非常嚴密，因此沒被攻擊，而當因中選會在選前即做好縝密的防護，當天其實沒因網路攻擊發生意外狀況，中選會的資安管理有成功防堵攻擊。Kolas表示，所幸這次在事前有預見可能的危險，中選會的資料庫、網站等都沒遭受損害，而在資訊時代，行政院希望在這波中選會的選務檢討中，不只針對有形被詬病的排隊投票問題，無形的選務如資安防護，也會進行檢討、加強。（林惟崧、鄭鴻達／台北報導）出版：1454更新：1942（新增行政院說法） \\xa0想知道更多，一定要看……【獨家】投票日遭境外駭客攻擊\\u3000中選會網站險癱瘓看了這則新聞的人，也看了……老里長險勝1票\\u3000美女里長候選人要求驗票發祭品囉！\\u3000韓總雞排逾千份這邊領吳敦義邀15位縣市首長積極參與決策\\u3000「黨意與民意呼吸」'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選擇文章，把文章分成句子\n",
    "docNum = 31\n",
    "testDoc = news100['content'][docNum]\n",
    "testTitle = news100['title'][docNum]\n",
    "\n",
    "def splitSentences(document):\n",
    "    #for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "    for sent in re.findall(u'[^!?。\\!\\?]+[!?。\\!\\?]?', document, flags=re.U):\n",
    "        yield sent\n",
    "\n",
    "sentenceDoc = []\n",
    "for sent in list(splitSentences(testDoc))[:-1]:\n",
    "    sentenceDoc.append(' '.join(jieba.cut(sent)))\n",
    "\n",
    "# 原本文章\n",
    "testDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今年 九 合一 大選 投票 狀況 多 ， 據 了解 ， 中選會 官方網站 在 投票 日 當天 曾 遭到 來自 境外 大規模 分散式 阻斷 服務 （ DD0oS ） 攻擊 ， 所幸 在 負責 網路 維護 中華電信 應變 下 ， 未 進一步 造成 影響 。',\n",
       " '中選會 副 主委 陳 朝 建今 證實 曾 遭 駭客 攻擊 ， 不過 計票 網路 是 封閉系統 ， 並非 公開 網路 ， 無法 由 外界 接取 ， 因此 沒有 任何 影響 ， 境外 網路 干擾 也 並未 影響 投票 進程 。',\n",
       " '行政院 發言人 Kolas   Yotaka （ 谷辣斯 ・ 尤 達卡 ） 表示 ， 中選會 在 選前 即 做好 縝密 防護 ， 因此 成功 防堵 攻擊 ， 接下來 將 針對 有形 的 選務 如 投票 排隊 問題 ， 以及 無形 的 選務 如資安 防護 進行 檢討 。',\n",
       " '不過 ， 投票 日 當天 開票 進程 緩慢 ， 台北市 選情 更 創下 史上 第一次 開票 長達 9 小時 的 紀錄 ， 陳 朝建 說明 ， 開票 時間 長 是 因為 投票 時間 延後 ， 計票 不 受 駭客 影響 ， 開票 也 自然 不受 此 影響 。',\n",
       " '據 內部消息 指出 ， 投票 日 當天 曾 出現 從 境外 進行 分散式 阻斷 服務 （ DDoS ） 攻擊 ， 企圖 癱瘓 中選會 網站 ， 不過 在 中華電信 、 中選會 緊急 處理 後 ， 阻斷 來自 國外 異常 流量 ， 在 短時間 獲得 解決 ， 一般 民眾並 沒有 發現 特殊 異樣 。',\n",
       " 'NCC 及 中華電信 暫未 對此 做 進一步 說明 。',\n",
       " '陳 朝建 表示 ， 受 攻擊 的 是 中選會 官方網站 ， 當日 開票 的 計票 網路 是 獨立 的 封閉 網路 ， 並非 公開 網路 ， 無法 由 外界 接取 ， 完全 未有 任何 影響 ； 中華電信 團隊 也 是 24 小時 監控 與 防護 作業 ， 也 沒有 受到 任何 影響 。',\n",
       " '陳 朝建 說 ， 受到 攻擊 的 官網 ， 當日 皆 有 隨時 防衛 ， 中華電信 團隊 24 小時 監控 與 防護 作業 ； 故 當日 遭到 來自 境外 大規模 分散式 阻斷 服務 （ DD0oS ） 攻擊 時 ， 在 負責 網路 維護 中華電信 的 緊急 應變 下 ， 並 沒有 受到 影響 。',\n",
       " '另外 ， Kolas 表示 ， 中選會 的 官網 在 選舉 當天 上午 6 時 56 分至 7 時 01 分 期間 ， 遭受 大規模 境外 DDoS 攻擊 ， 而 中選會 的 系統 是 封閉系統 ， 防護 非常 嚴密 ， 因此 沒 被 攻擊 ， 而 當因 中選會 在 選前 即 做好 縝密 的 防護 ， 當天 其實 沒因 網路 攻擊 發生意外 狀況 ， 中選會 的 資安 管理 有 成功 防堵 攻擊 。',\n",
       " 'Kolas 表示 ， 所幸 這次 在 事前 有 預見 可能 的 危險 ， 中選會 的 資料庫 、 網站 等 都 沒 遭受 損害 ， 而 在 資訊 時代 ， 行政院 希望 在 這波 中選會 的 選務 檢討 中 ， 不 只 針對 有形 被 詬病 的 排隊 投票 問題 ， 無形 的 選務 如資安 防護 ， 也 會 進行 檢討 、 加強 。']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分詞後的文章\n",
    "sentenceDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x6500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把文章轉換成vector\n",
    "doctor_tfidf = vect.transform(sentenceDoc)\n",
    "doctor_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算句子相似度用來再畫圖輪\n",
    "- 把文章轉換成 vector 以後可以用來計算句子之間的相似度依照TF-IDF分數(cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創造空的 similarity matrix (相似度矩陣)\n",
    "sim_matrix = np.zeros([len(sentenceDoc), len(sentenceDoc)])\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.28932826, 0.07071838, 0.12412599, 0.40618577,\n",
       "        0.16729712, 0.28698711, 0.5139728 , 0.28538874, 0.09787338],\n",
       "       [0.28932826, 0.        , 0.05486499, 0.21259262, 0.10609694,\n",
       "        0.        , 0.55933654, 0.20667443, 0.26784045, 0.04087918],\n",
       "       [0.07071838, 0.05486499, 0.        , 0.02112692, 0.07366853,\n",
       "        0.        , 0.1397056 , 0.12862096, 0.42785049, 0.53365807],\n",
       "       [0.12412599, 0.21259262, 0.02112692, 0.        , 0.04841702,\n",
       "        0.03906373, 0.24261615, 0.0947003 , 0.03329598, 0.01936448],\n",
       "       [0.40618577, 0.10609694, 0.07366853, 0.04841702, 0.        ,\n",
       "        0.08106393, 0.08295373, 0.31567354, 0.23925864, 0.10862471],\n",
       "       [0.16729712, 0.        , 0.        , 0.03906373, 0.08106393,\n",
       "        0.        , 0.08336997, 0.14799011, 0.        , 0.        ],\n",
       "       [0.28698711, 0.55933654, 0.1397056 , 0.24261615, 0.08295373,\n",
       "        0.08336997, 0.        , 0.4983989 , 0.25687401, 0.07563192],\n",
       "       [0.5139728 , 0.20667443, 0.12862096, 0.0947003 , 0.31567354,\n",
       "        0.14799011, 0.4983989 , 0.        , 0.2992971 , 0.0388818 ],\n",
       "       [0.28538874, 0.26784045, 0.42785049, 0.03329598, 0.23925864,\n",
       "        0.        , 0.25687401, 0.2992971 , 0.        , 0.2234744 ],\n",
       "       [0.09787338, 0.04087918, 0.53365807, 0.01936448, 0.10862471,\n",
       "        0.        , 0.07563192, 0.0388818 , 0.2234744 , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 cosine similarity 計算文章句子之間的相似度，並把空的 similarity matrix 填滿\n",
    "for i in range(len(sentenceDoc)):\n",
    "    for j in range(len(sentenceDoc)):\n",
    "        if i != j:\n",
    "            sim_matrix[i][j] = cosine_similarity(doctor_tfidf[i], doctor_tfidf[j])[0,0]\n",
    "            #sim_matrix[i][j] = cosine_similarity(doctor_tfidf[i].reshape(1,100), doctor_tfidf[j].reshape(1,100))[0,0]\n",
    "\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 similarity matrix 變成成圖論（節點代表句子，線段代表相似度），再用 PageRank 演算法找出最重要的句子\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 長文摘要\n",
    "- 把文章轉換成圖論，找出最重要的節點/句子創建摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標題: 《中選會證實投票日遭駭客攻擊　行政院：成功防堵》\n",
      "Sentence 0\n",
      " 今年九合一大選投票狀況多，據了解，中選會官方網站在投票日當天曾遭到來自境外大規模分散式阻斷服務（DD0oS）攻擊，所幸在負責網路維護中華電信應變下，未進一步造成影響。\n",
      "Sentence 6\n",
      " 陳朝建表示，受攻擊的是中選會官方網站，當日開票的計票網路是獨立的封閉網路，並非公開網路，無法由外界接取，完全未有任何影響；中華電信團隊也是24小時監控與防護作業，也沒有受到任何影響。\n",
      "Sentence 7\n",
      " 陳朝建說，受到攻擊的官網，當日皆有隨時防衛，中華電信團隊24小時監控與防護作業；故當日遭到來自境外大規模分散式阻斷服務（DD0oS）攻擊時，在負責網路維護中華電信的緊急應變下，並沒有受到影響。\n"
     ]
    }
   ],
   "source": [
    "# 找出 PageRank 算最重要的三個句子做摘要\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentenceDoc)), reverse=True)\n",
    "\n",
    "summarySentences = []\n",
    "for i in range(3):\n",
    "    summarySentences.append(ranked_sentences[i][1])\n",
    "\n",
    "print('標題: ' + '《' + testTitle + '》')\n",
    "for sentence in sorted(summarySentences):\n",
    "    print('Sentence ' + str(sentenceDoc.index(sentence)) + '\\n', sentence.replace(' ','').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**會提高摘要效果的改進**\n",
    "1. 更進階的方法計算句子的重要性\n",
    "    - word embeddings (e.g. word2vec)\n",
    "2. 更好的分詞/分句子，詞庫，和語料庫會有更好的效果/摘要\n",
    "3. 利用同義詞加強詞庫\n",
    "4. 不管是单词还是句子，相似度计算太表面了，没有涉及到语义层面，效果肯定不会太好。（原因是sentence vector的质量不高，但是word vector的质量非常高）\n",
    "5. 句子得分模型太过粗糙了，没有考虑feature，得到的效果感觉不会很好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
